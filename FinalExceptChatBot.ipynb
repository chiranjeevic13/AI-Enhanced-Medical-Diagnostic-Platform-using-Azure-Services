{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f3348a-9e68-481d-adc7-0dae4cdcd438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:msrest.universal_http.requests:Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Enter symptoms directly\n",
      "2. Upload a file (txt, pdf, docx, pptx, xlsx, csv, json, xml)\n",
      "3. Upload an audio file\n",
      "4. Record live audio\n",
      "5. Upload an image\n",
      "Enter your choice (1, 2, 3, 4, or 5):  1\n",
      "Enter your symptoms (in any language):  happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): translate.google.com:443\n",
      "DEBUG:urllib3.connectionpool:https://translate.google.com:443 \"GET /m?tl=en&sl=fi&q=happy HTTP/11\" 200 None\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://p2languageservice.cognitiveservices.azure.com/language/:analyze-text?api-version=2023-04-01'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '132'\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': 'f87f25c9-7f65-11ef-8101-914a55c7109a'\n",
      "    'User-Agent': 'azsdk-python-ai-textanalytics/5.3.0 Python/3.9.11 (Windows-10-10.0.22631-SP0)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "A body is sent with the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: Finnish\n",
      "\n",
      "Translated Text: happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): p2languageservice.cognitiveservices.azure.com:443\n",
      "DEBUG:urllib3.connectionpool:https://p2languageservice.cognitiveservices.azure.com:443 \"POST /language/:analyze-text?api-version=2023-04-01 HTTP/11\" 200 206\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '206'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'csp-billing-usage': 'REDACTED'\n",
      "    'x-envoy-upstream-service-time': '6'\n",
      "    'Set-Cookie': 'REDACTED'\n",
      "    'apim-request-id': 'a9514a49-2a65-4158-b1d2-b0c554f7801c'\n",
      "    'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload'\n",
      "    'x-content-type-options': 'nosniff'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Mon, 30 Sep 2024 19:55:41 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://p2languageservice.cognitiveservices.azure.com/language/:analyze-text?api-version=2023-04-01'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '166'\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': 'f9076c47-7f65-11ef-9689-914a55c7109a'\n",
      "    'User-Agent': 'azsdk-python-ai-textanalytics/5.3.0 Python/3.9.11 (Windows-10-10.0.22631-SP0)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "A body is sent with the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://p2languageservice.cognitiveservices.azure.com:443 \"POST /language/:analyze-text?api-version=2023-04-01 HTTP/11\" 200 351\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '351'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'csp-billing-usage': 'REDACTED'\n",
      "    'x-envoy-upstream-service-time': '12'\n",
      "    'Set-Cookie': 'REDACTED'\n",
      "    'apim-request-id': '4772be60-c423-4be6-8d39-3e1fd3137e7a'\n",
      "    'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload'\n",
      "    'x-content-type-options': 'nosniff'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Mon, 30 Sep 2024 19:55:41 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n",
      "The input statement is positive. No disease identification needed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import time \n",
    "import pyaudio\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text import TextTranslationClient\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from PyPDF2 import PdfFileReader\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from xlrd import open_workbook\n",
    "from csv import reader\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# Enable logging for debugging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# Azure Custom Vision setup\n",
    "ENDPOINT = \"https://p2ic-prediction.cognitiveservices.azure.com/\"\n",
    "prediction_key = \"64bfcfb8949d484fae93402fa55e1425\"\n",
    "project_id = \"c450da0e-a657-4c10-a1a4-f8ed35e8d40a\"\n",
    "publish_name = \"DiseaseClassification\"\n",
    "\n",
    "# Create CustomVisionPredictionClient\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n",
    "predictor = CustomVisionPredictionClient(ENDPOINT, credentials)\n",
    "\n",
    "# Azure credentials\n",
    "AI_SERVICE_ENDPOINT = \"https://p2languageservice.cognitiveservices.azure.com/\"\n",
    "AI_SERVICE_KEY = \"741f912d0fb040d1afb1a3ca8b4f64d9\"\n",
    "TRANSLATION_ENDPOINT = \"https://api.cognitive.microsofttranslator.com/\"\n",
    "TRANSLATION_KEY = \"256e3786ca11469980ddae934f5944e5\"\n",
    "SPEECH_KEY = \"872d1354c89e49158adb2b10b0c42493\"\n",
    "SPEECH_REGION = \"eastus\"\n",
    "\n",
    "# Load all the datasets into a list of DataFrames\n",
    "datasets = [\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/Lifestyle-Related Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/disease_data.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/Environmental Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/Idiopathic.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/Neoplastic Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/non-infectious diseases_data.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/Nutritional Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P2/Psychiatric and Neurological Disorders.csv\"),\n",
    "    pd.read_csv(\"D:\\AI-Projects\\FinalProject\\P2\\Rare Diseases.csv\")\n",
    "]\n",
    "\n",
    "# Combine all datasets into one DataFrame for easier searching\n",
    "combined_df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Clean and normalize text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    text = ' '.join(dict.fromkeys(text.split()))  # Remove duplicate words\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def authenticate_azure_translation():\n",
    "    credential = AzureKeyCredential(TRANSLATION_KEY)\n",
    "    client = TextTranslationClient(endpoint=TRANSLATION_ENDPOINT, credential=credential)\n",
    "    return client\n",
    "\n",
    "def authenticate_azure_nlp():\n",
    "    credential = AzureKeyCredential(AI_SERVICE_KEY)\n",
    "    client = TextAnalyticsClient(endpoint=AI_SERVICE_ENDPOINT, credential=credential)\n",
    "    return client\n",
    "\n",
    "def detect_language(text):\n",
    "    client = authenticate_azure_nlp()\n",
    "    try:\n",
    "        response = client.detect_language(documents=[text])\n",
    "        primary_language = response[0].primary_language.iso6391_name\n",
    "        return primary_language\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "def translate_to_english(text):\n",
    "    client = authenticate_azure_translation()\n",
    "    try:\n",
    "        # Send the request to the Azure Translation service\n",
    "        response = client.translate(\n",
    "            body=[{\n",
    "                \"text\": text\n",
    "            }],\n",
    "            to_language=[\"en\"]\n",
    "        )\n",
    "        \n",
    "        # Extract the translated text from the response\n",
    "        if response and 'translations' in response[0]:\n",
    "            translated_text = response[0]['translations'][0]['text']\n",
    "            print(f\"Translated Text: {translated_text}\")\n",
    "            return translated_text\n",
    "        else:\n",
    "            print(\"No translations found in the response.\")\n",
    "            return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text\n",
    "\n",
    "# Translate text to English if not already in English\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        user_lang = detect(text)\n",
    "        print(f\"Detected Language: {lang_code_to_full.get(user_lang, user_lang)}\")\n",
    "        if user_lang != 'en':\n",
    "            text = GoogleTranslator(source=user_lang, target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating to English: {e}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract key phrases from user input text using Azure Text Analytics\n",
    "def extract_key_phrases(text):\n",
    "    client = authenticate_azure_nlp()\n",
    "    response = client.extract_key_phrases(documents=[text])[0]\n",
    "    return response.key_phrases\n",
    "\n",
    "# Find diseases based on extracted key phrases (symptoms)\n",
    "def find_disease_by_symptoms(key_phrases, combined_df):\n",
    "    key_phrases_cleaned = [clean_text(phrase) for phrase in key_phrases]\n",
    "    matched_diseases = combined_df[combined_df['Symptoms'].apply(lambda x: any(phrase in clean_text(x) for phrase in key_phrases_cleaned))]\n",
    "    return matched_diseases\n",
    "\n",
    "# Prioritize diseases by severity and frequency of key phrase matches\n",
    "def prioritize_combined(matched_diseases, key_phrases):\n",
    "    severity_priority = {'Mild': 1, 'Moderate': 2, 'Severe': 3}\n",
    "    matched_diseases.loc[:, 'Severity Priority'] = matched_diseases['Severity Level'].map(severity_priority)\n",
    "    \n",
    "    def count_matches(symptoms):\n",
    "        cleaned_symptoms = clean_text(symptoms)\n",
    "        return sum(phrase in cleaned_symptoms for phrase in key_phrases)\n",
    "    \n",
    "    matched_diseases.loc[:, 'Match Count'] = matched_diseases['Symptoms'].apply(count_matches)\n",
    "    prioritized_diseases = matched_diseases.sort_values(by=['Match Count', 'Severity Priority'], ascending=[False, False])\n",
    "    return prioritized_diseases\n",
    "\n",
    "# Print disease information in a formatted way\n",
    "def print_disease_info(disease, language):\n",
    "    disease_info = (\n",
    "        f\"\\n--- Disease Information ---\\n\"\n",
    "        f\"Disease Name: {disease['Disease Name']}\\n\"\n",
    "        f\"Severity Level: {disease['Severity Level']}\\n\"\n",
    "        f\"Symptoms: {disease['Symptoms']}\\n\"\n",
    "        f\"Recommended Medications: {disease['Recommended Medications']}\\n\"\n",
    "        f\"Required Food: {disease['Required Food']}\\n\"\n",
    "        f\"Safety Precautions: {disease['Safety Precautions']}\\n\"\n",
    "        f\"Recommended Doctor: {disease['Recommended Doctor']}\\n\"\n",
    "        f\"Treatment Plan: {disease['Treatment Plan']}\\n\"\n",
    "        f\"Follow-Up Recommendations: {disease['Follow-Up Recommendations']}\\n\"\n",
    "        f\"Patient Education: {disease['Patient Education']}\\n\"\n",
    "        f\"Recovery Time: {disease['Recovery Time']}\"\n",
    "    )\n",
    "    if language != 'en':\n",
    "        translator = GoogleTranslator(source='en', target=language)\n",
    "        disease_info = translator.translate(disease_info)\n",
    "    print(disease_info)\n",
    "\n",
    "# Detect language and perform sentiment analysis\n",
    "def detect_language_and_sentiment(text):\n",
    "    client = authenticate_azure_nlp()\n",
    "    \n",
    "    # Detect Language\n",
    "    detected_language = client.detect_language(documents=[text])[0]\n",
    "    detected_lang_code = detected_language.primary_language.iso6391_name\n",
    "    print(f\"Detected Language: {lang_code_to_full.get(detected_lang_code, detected_lang_code)}\")\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    sentiment_response = client.analyze_sentiment(documents=[text])[0]\n",
    "    sentiment = sentiment_response.sentiment\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    return sentiment, detected_lang_code\n",
    "\n",
    "# Language code to full name dictionary\n",
    "lang_code_to_full = {\n",
    "    'en': 'English',\n",
    "    'fr': 'French',\n",
    "    'es': 'Spanish',\n",
    "    'de': 'German',\n",
    "    'it': 'Italian',\n",
    "    'pt': 'Portuguese',\n",
    "    'zh': 'Chinese',\n",
    "    'ja': 'Japanese',\n",
    "    'ko': 'Korean',\n",
    "    'ar': 'Arabic',\n",
    "    'ru': 'Russian',\n",
    "    'hi': 'Hindi',\n",
    "    'bn': 'Bengali',\n",
    "    'pa': 'Punjabi',\n",
    "    'jv': 'Javanese',\n",
    "    'ml': 'Malayalam',\n",
    "    'tr': 'Turkish',\n",
    "    'vi': 'Vietnamese',\n",
    "    'th': 'Thai',\n",
    "    'sv': 'Swedish',\n",
    "    'da': 'Danish',\n",
    "    'no': 'Norwegian',\n",
    "    'fi': 'Finnish',\n",
    "    'cs': 'Czech',\n",
    "    'pl': 'Polish',\n",
    "    'uk': 'Ukrainian',\n",
    "    'el': 'Greek',\n",
    "    'he': 'Hebrew',\n",
    "    'te': 'Telugu',\n",
    "    'ur': 'Urdu',\n",
    "    'kn': 'Kannada',\n",
    "    'ta': 'Tamil'\n",
    "}\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    # Extract text from .txt files\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    # Extract text from .pdf files\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PdfFileReader(file)\n",
    "            text = \"\"\n",
    "            for page_num in range(reader.numPages):\n",
    "                text += reader.getPage(page_num).extract_text()\n",
    "            return text\n",
    "    \n",
    "    # Extract text from .docx files\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = Document(file_path)\n",
    "        text = \"\"\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text\n",
    "        return text\n",
    "\n",
    "    # Extract text from .pptx files\n",
    "    elif file_path.endswith('.pptx'):\n",
    "        presentation = Presentation(file_path)\n",
    "        text = \"\"\n",
    "        for slide in presentation.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, 'text'):\n",
    "                    text += shape.text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    # Extract text from .xlsx files\n",
    "    elif file_path.endswith('.xlsx'):\n",
    "        df = pd.read_excel(file_path)\n",
    "        text = \"\"\n",
    "        for column in df.columns:\n",
    "            text += df[column].astype(str).str.cat(sep='\\n')\n",
    "        return text\n",
    "\n",
    "    # Extract text from .csv files\n",
    "    elif file_path.endswith('.csv'):\n",
    "        text = \"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            csv_reader = reader(file)\n",
    "            for row in csv_reader:\n",
    "                text += ' '.join(row) + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    # Extract text from .json files\n",
    "    elif file_path.endswith('.json'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return json.dumps(data, indent=2)\n",
    "    \n",
    "    # Extract text from .xml files\n",
    "    elif file_path.endswith('.xml'):\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        return ET.tostring(root, encoding='unicode')\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def record_live_audio(output_file_path, duration=5):\n",
    "    \"\"\"Record live audio and save it as a WAV file.\"\"\"\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=44100, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    \n",
    "    for _ in range(int(44100 / 1024 * duration)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    \n",
    "    print(\"Recording finished.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    \n",
    "    with wave.open(output_file_path, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(44100)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "    print(f\"Recorded audio saved to {output_file_path}\")\n",
    "    \n",
    "    return output_file_path\n",
    "\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_path)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    \n",
    "    all_text = []\n",
    "    recognition_finished = False\n",
    "\n",
    "    def recognized_callback(evt):\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            print(f\"Recognized: {evt.result.text}\")\n",
    "            all_text.append(evt.result.text)\n",
    "        elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"No speech could be recognized.\")\n",
    "\n",
    "    def session_stopped_callback(evt):\n",
    "        nonlocal recognition_finished\n",
    "        recognition_finished = True\n",
    "        print(\"Recognition session stopped.\")\n",
    "\n",
    "    speech_recognizer.recognized.connect(recognized_callback)\n",
    "    speech_recognizer.session_stopped.connect(session_stopped_callback)\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "    try:\n",
    "        # Wait until recognition is finished\n",
    "        while not recognition_finished:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        # Stop recognition when Ctrl+C is pressed\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "    \n",
    "    # Ensure continuous recognition has stopped\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    return \" \".join(all_text)\n",
    "\n",
    "\n",
    "# Classify image using Azure Custom Vision\n",
    "def classify_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_data:\n",
    "        results = predictor.classify_image(project_id, publish_name, image_data.read())\n",
    "    \n",
    "    logging.info(f\"Predictions for image: {image_path}\")\n",
    "    for prediction in results.predictions:\n",
    "        logging.info(f\"Tag: {prediction.tag_name}, Probability: {prediction.probability:.2f}\")\n",
    "\n",
    "    best_prediction = max(results.predictions, key=lambda p: p.probability)\n",
    "    return best_prediction.tag_name if best_prediction.probability > 0 else None\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Provide user with options\n",
    "    choice = input(\"Choose an option:\\n1. Enter symptoms directly\\n2. Upload a file (txt, pdf, docx, pptx, xlsx, csv, json, xml)\\n3. Upload an audio file\\n4. Record live audio\\n5. Upload an image\\nEnter your choice (1, 2, 3, 4, or 5): \").strip()\n",
    "\n",
    "    if choice == '1':\n",
    "        # Option 1: User enters symptoms directly\n",
    "        user_input = input(\"Enter your symptoms (in any language): \").strip()\n",
    "        \n",
    "        # Translate text to English if needed\n",
    "        translated_text = translate_to_english(user_input)\n",
    "        print(f\"\\nTranslated Text: {translated_text}\")\n",
    "        \n",
    "        # Detect language and perform sentiment analysis\n",
    "        sentiment, detected_language = detect_language_and_sentiment(translated_text)\n",
    "        \n",
    "        # If sentiment is positive, acknowledge and skip disease identification\n",
    "        if sentiment == 'positive':\n",
    "            print(\"The input statement is positive. No disease identification needed.\")\n",
    "            return\n",
    "        \n",
    "        # Extract key phrases (symptoms) from translated text\n",
    "        key_phrases = extract_key_phrases(translated_text)\n",
    "        print(f\"\\nExtracted Key Phrases (Symptoms): {key_phrases}\")\n",
    "        \n",
    "        # Find diseases based on the extracted symptoms\n",
    "        matched_diseases = find_disease_by_symptoms(key_phrases, combined_df)\n",
    "        \n",
    "        # Prioritize diseases\n",
    "        if not matched_diseases.empty:\n",
    "            prioritized_diseases = prioritize_combined(matched_diseases, key_phrases)\n",
    "            top_disease = prioritized_diseases.iloc[0]\n",
    "            \n",
    "            # Ask user for their preferred output language\n",
    "            preferred_language = input(\"Enter your preferred output language (e.g., 'fr' for French, 'es' for Spanish): \").strip().lower()\n",
    "            if preferred_language not in lang_code_to_full:\n",
    "                preferred_language = 'en'  # Default to English if an invalid code is provided\n",
    "            \n",
    "            print_disease_info(top_disease, preferred_language)\n",
    "        else:\n",
    "            print(\"\\nNo matching diseases found based on the provided symptoms.\")\n",
    "    \n",
    "    elif choice == '2':\n",
    "        # Option 2: User uploads a file\n",
    "        file_path = input(\"Enter the path to your file (txt, pdf, docx, pptx, xlsx, csv, json, xml): \").strip()\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"The specified file does not exist.\")\n",
    "            return\n",
    "        \n",
    "        # Extract text from the uploaded file\n",
    "        file_text = extract_text_from_file(file_path)\n",
    "        print(f\"\\nExtracted Text: {file_text}\")\n",
    "        \n",
    "        # Translate text to English if needed\n",
    "        translated_text = translate_to_english(file_text)\n",
    "        print(f\"\\nTranslated Text: {translated_text}\")\n",
    "        \n",
    "        # Detect language and perform sentiment analysis\n",
    "        sentiment, detected_language = detect_language_and_sentiment(translated_text)\n",
    "        \n",
    "        # If sentiment is positive, acknowledge and skip disease identification\n",
    "        if sentiment == 'positive':\n",
    "            print(\"The input statement is positive. No disease identification needed.\")\n",
    "            return\n",
    "        \n",
    "        # Extract key phrases (symptoms) from translated text\n",
    "        key_phrases = extract_key_phrases(translated_text)\n",
    "        print(f\"\\nExtracted Key Phrases (Symptoms): {key_phrases}\")\n",
    "        \n",
    "        # Find diseases based on the extracted symptoms\n",
    "        matched_diseases = find_disease_by_symptoms(key_phrases, combined_df)\n",
    "        \n",
    "        # Prioritize diseases\n",
    "        if not matched_diseases.empty:\n",
    "            prioritized_diseases = prioritize_combined(matched_diseases, key_phrases)\n",
    "            top_disease = prioritized_diseases.iloc[0]\n",
    "            \n",
    "            # Ask user for their preferred output language\n",
    "            preferred_language = input(\"Enter your preferred output language (e.g., 'fr' for French, 'es' for Spanish): \").strip().lower()\n",
    "            if preferred_language not in lang_code_to_full:\n",
    "                preferred_language = 'en'  # Default to English if an invalid code is provided\n",
    "            \n",
    "            print_disease_info(top_disease, preferred_language)\n",
    "        else:\n",
    "            print(\"\\nNo matching diseases found based on the provided symptoms.\")\n",
    "    \n",
    "    elif choice == '3':\n",
    "        # Option 3: User uploads an audio file\n",
    "        audio_file_path = input(\"Enter the path to the audio file for transcription: \").strip()\n",
    "        \n",
    "        if not os.path.exists(audio_file_path):\n",
    "            print(\"The specified audio file does not exist.\")\n",
    "            return\n",
    "        \n",
    "        # Transcribe audio to text\n",
    "        transcription = transcribe_audio(audio_file_path)\n",
    "        print(f\"\\nTranscription: {transcription}\")\n",
    "        \n",
    "        # Translate text to English if needed\n",
    "        translated_text = translate_to_english(transcription)\n",
    "        print(f\"\\nTranslated Text: {translated_text}\")\n",
    "        \n",
    "        # Detect language and perform sentiment analysis\n",
    "        sentiment, detected_language = detect_language_and_sentiment(translated_text)\n",
    "        \n",
    "        # If sentiment is positive, acknowledge and skip disease identification\n",
    "        if sentiment == 'positive':\n",
    "            print(\"The input statement is positive. No disease identification needed.\")\n",
    "            return\n",
    "        \n",
    "        # Extract key phrases (symptoms) from translated text\n",
    "        key_phrases = extract_key_phrases(translated_text)\n",
    "        print(f\"\\nExtracted Key Phrases (Symptoms): {key_phrases}\")\n",
    "        \n",
    "        # Find diseases based on the extracted symptoms\n",
    "        matched_diseases = find_disease_by_symptoms(key_phrases, combined_df)\n",
    "        \n",
    "        # Prioritize diseases\n",
    "        if not matched_diseases.empty:\n",
    "            prioritized_diseases = prioritize_combined(matched_diseases, key_phrases)\n",
    "            top_disease = prioritized_diseases.iloc[0]\n",
    "            \n",
    "            # Ask user for their preferred output language\n",
    "            preferred_language = input(\"Enter your preferred output language (e.g., 'fr' for French, 'es' for Spanish): \").strip().lower()\n",
    "            if preferred_language not in lang_code_to_full:\n",
    "                preferred_language = 'en'  # Default to English if an invalid code is provided\n",
    "            \n",
    "            print_disease_info(top_disease, preferred_language)\n",
    "        else:\n",
    "            print(\"\\nNo matching diseases found based on the provided symptoms.\")\n",
    "    \n",
    "    elif choice == '4':\n",
    "        output_file_path = \"live_recorded_audio.mp3\"\n",
    "        duration = int(input(\"Enter the duration for live recording in seconds: \").strip())\n",
    "        final_live_record_path=record_live_audio(output_file_path, duration)\n",
    "        # Transcribe audio to text\n",
    "        transcription = transcribe_audio(final_live_record_path)\n",
    "        print(f\"\\nTranscription: {transcription}\")\n",
    "        \n",
    "        # Translate text to English if needed\n",
    "        translated_text = translate_to_english(transcription)\n",
    "        print(f\"\\nTranslated Text: {translated_text}\")\n",
    "        \n",
    "        # Detect language and perform sentiment analysis\n",
    "        sentiment, detected_language = detect_language_and_sentiment(translated_text)\n",
    "        \n",
    "        # If sentiment is positive, acknowledge and skip disease identification\n",
    "        if sentiment == 'positive':\n",
    "            print(\"The input statement is positive. No disease identification needed.\")\n",
    "            return\n",
    "        \n",
    "        # Extract key phrases (symptoms) from translated text\n",
    "        key_phrases = extract_key_phrases(translated_text)\n",
    "        print(f\"\\nExtracted Key Phrases (Symptoms): {key_phrases}\")\n",
    "        \n",
    "        # Find diseases based on the extracted symptoms\n",
    "        matched_diseases = find_disease_by_symptoms(key_phrases, combined_df)\n",
    "        \n",
    "        # Prioritize diseases\n",
    "        if not matched_diseases.empty:\n",
    "            prioritized_diseases = prioritize_combined(matched_diseases, key_phrases)\n",
    "            top_disease = prioritized_diseases.iloc[0]\n",
    "            \n",
    "            # Ask user for their preferred output language\n",
    "            preferred_language = input(\"Enter your preferred output language (e.g., 'fr' for French, 'es' for Spanish): \").strip().lower()\n",
    "            if preferred_language not in lang_code_to_full:\n",
    "                preferred_language = 'en'  # Default to English if an invalid code is provided\n",
    "            \n",
    "            print_disease_info(top_disease, preferred_language)\n",
    "        else:\n",
    "            print(\"\\nNo matching diseases found based on the provided symptoms.\")\n",
    "\n",
    "    elif choice == '5':\n",
    "        image_path = input(\"Enter the path to the image file for classification: \").strip()\n",
    "        if not os.path.exists(image_path):\n",
    "            print(\"The specified image file does not exist.\")\n",
    "            return\n",
    "\n",
    "        disease_tag = classify_image(image_path)\n",
    "        if disease_tag:\n",
    "            print(f\"\\nClassified Disease Tag: {disease_tag}\")\n",
    "            matched_diseases = combined_df[combined_df['Disease Name'].str.lower() == disease_tag.lower()]  # Use the correct column for matching\n",
    "\n",
    "\n",
    "            if not matched_diseases.empty:\n",
    "                prioritized_diseases = prioritize_combined(matched_diseases, [])\n",
    "                top_disease = prioritized_diseases.iloc[0]\n",
    "                \n",
    "                preferred_language = input(\"Enter your preferred output language (e.g., 'fr' for French): \").strip().lower()\n",
    "                if preferred_language not in lang_code_to_full:\n",
    "                    preferred_language = 'en'\n",
    "                print_disease_info(top_disease, preferred_language)\n",
    "            else:\n",
    "                print(\"\\nNo matching diseases found for the classified tag.\")\n",
    "        else:\n",
    "            print(\"No disease could be classified from the image.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 1, 2, 3, 4, or 5.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "#D:\\AI-Projects\\FinalProject\\P2\\ImageClassification\\BrainTumorAndChestX_RayDatasets\\Training\\covid19\\COVID19(2).jpg\n",
    "#D:\\AI-Projects\\FinalProject\\P2\\ImageClassification\\BrainTumorAndChestX_RayDatasets\\Training\\glioma\\Tr-gl_0012.jpg\n",
    "#D:\\AI-Projects\\FinalProject\\P2\\ImageClassification\\BrainTumorAndChestX_RayDatasets\\Training\\meningioma\\Tr-me_0012.jpg\n",
    "#D:\\AI-Projects\\FinalProject\\P2\\ImageClassification\\BrainTumorAndChestX_RayDatasets\\Training\\notumor\\Tr-no_0045.jpg\n",
    "#D:\\AI-Projects\\FinalProject\\P2\\ImageClassification\\BrainTumorAndChestX_RayDatasets\\Training\\pituitary\\Tr-pi_0033.jpg    \n",
    "#D:\\AI-Projects\\FinalProject\\P2\\Patientpk.wav\n",
    "#D:\\AI-Projects\\FinalProject\\P2\\RecieptInJapanese.txt\n",
    "#Последние несколько дней я испытываю настоящие страдания. Всё вокруг кажется серым и мрачным, и я чувствую себя одиноко и подавленно. Каждое утро начинается с тяжёлого чувства тревоги, и с каждым днём становится всё труднее справляться с этим. Надеюсь, что скоро наступят лучше времена и я смогу найти покой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf25651-0fd0-4912-8578-c618b6b224d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805bb887-e156-41c9-a076-e10baf29df5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
